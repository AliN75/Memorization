{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8b746034-60cf-455e-b256-20ddee67836a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/anaseh_umass_edu/anaconda3/envs/test/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#Import the libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import nltk\n",
    "import re\n",
    "import csv\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from tqdm import tqdm\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import GPT2Tokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, AutoConfig, GPTNeoForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import os\n",
    "import openai\n",
    "from re import search\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f087aada-7e56-4d26-b0a3-9b8390005f72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1301a59d-d9e7-4c44-bbc9-32dce6a562d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95089e34-416a-468e-9869-8c2c207793c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /home/anaseh_umass_edu/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bbb34659-ddf6-47d4-b951-55b46a47047c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)lve/main/config.json: 100%|██████████| 718/718 [00:00<00:00, 161kB/s]\n",
      "Downloading (…)olve/main/vocab.json: 100%|██████████| 1.04M/1.04M [00:00<00:00, 3.93MB/s]\n",
      "Downloading (…)olve/main/merges.txt: 100%|██████████| 456k/456k [00:00<00:00, 3.63MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 1.36M/1.36M [00:00<00:00, 4.66MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Modify the tokenizer if needed.\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2-medium\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bde581fb-8b91-4e90-b5c9-e15c47c45557",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)\"pytorch_model.bin\";: 100%|██████████| 1.52G/1.52G [00:32<00:00, 47.1MB/s]\n",
      "Downloading (…)neration_config.json: 100%|██████████| 124/124 [00:00<00:00, 56.8kB/s]\n"
     ]
    }
   ],
   "source": [
    "# Change the config if needed.\n",
    "config = AutoConfig.from_pretrained(\"gpt2-medium\")\n",
    "\n",
    "# Change the tokenizer if needed.\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2-medium\", config=config, pad_token_id=tokenizer.eos_token_id).cuda()\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56668dc-d140-438b-88e2-5ee11a37810e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modify the path if needed\n",
    "model.load_state_dict(torch.load('gpt_2_test/checkpoint-100/pytorch_model.bin'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15065684-f735-45c3-a32c-ef026a1b9685",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_similarity_check = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0fe0d82b-5a6a-4589-a617-d2cf344bb81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_score(text1, text2):\n",
    "    \n",
    "    #Compute embedding for both lists\n",
    "    embeddings1 = model_similarity_check.encode(text1, convert_to_tensor=True)\n",
    "    embeddings2 = model_similarity_check.encode(text2, convert_to_tensor=True)\n",
    "    \n",
    "    #Compute cosine-similarities\n",
    "    cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
    "    \n",
    "    return cosine_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd558454-603a-46e6-8f33-c7c2327d26a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "If the decoding algorithm is one of the greedy search or beam search, the generated text might be repetitive. \n",
    "Hence, we wrote this function to check if the generated text is repetitive. If so, we will remove that sample.\n",
    "\"\"\"\n",
    "\n",
    "def has_repetition(generated_text, window_size=10):\n",
    "    \"\"\"\n",
    "    Determines if there is repetitive text in a generated text using a sliding window of the specified size.\n",
    "    Returns True if there is any repetition and False otherwise.\n",
    "    \"\"\"\n",
    "    window_start = 0\n",
    "    window_end = window_size\n",
    "    while window_end < len(generated_text):\n",
    "        window_text = generated_text[window_start:window_end]\n",
    "        if window_text in generated_text[window_end:]:\n",
    "            return True\n",
    "        window_start += 1\n",
    "        window_end += 1\n",
    "    return False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa60fbf-1cfb-41d3-99d1-36e5b9b2a4b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "We initiate a dictionary to save values for different lengths. For each length, we save the number of samples of that length and the sum of the scores. In the end, we can compute the average score for each length.\n",
    "\"\"\"\n",
    "\n",
    "results = {}\n",
    "for i in range(100):\n",
    "    results[str(i)] = [0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d36890c6-5fa6-4b0f-89f5-c99ec9a2e712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the dataset\n",
    "df = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "728bedb6-d5d6-47a8-84f3-1ff83606dd04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>&lt;cs.CR&gt;:   An access control model called Zero...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>&lt;gr-qc&gt;:   Using quantum Riemannian geometry, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>&lt;gr-qc&gt;:   Noether's theorem identifies fundam...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>&lt;stat.ME&gt;:   Multilevel models (mixed-effect m...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                               text\n",
       "0           0  <cs.CR>:   An access control model called Zero...\n",
       "1           1  <gr-qc>:   Using quantum Riemannian geometry, ...\n",
       "2           2  <gr-qc>:   Noether's theorem identifies fundam...\n",
       "3           3  <stat.ME>:   Multilevel models (mixed-effect m..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "138906d0-a67d-43e4-9631-64e06bf40168",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "Thus, even a\n",
      "small amount of angular noncommutativity can be used to solve for a\n",
      "spherically-symmetric solution in 4D, with the $S^2$ at each $t,r$ a noncommutative fuzzy sphere, finding a dimension jump with solutions having thetime\n",
      "2\n",
      "Here we\n",
      "show that the surface gravity as naturally defined for Noether charges can be approximated\n",
      "by a Hamiltonian, and that this Hamiltonian can be approximated by a Noether charge.\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "Minimax lower bounds are established to demonstrate the\n",
      "necessity of structural assumptions such as sparsity conditions on the\n",
      "discriminating direction and differential graph for the possible construction\n",
      "of consistent quadratic discriminant models.\n",
      "7\n",
      "8\n",
      "9\n",
      "The existing inference\n",
      "methods for the causal effect with invalid instruments are based on the assumption that the\n",
      "\n",
      "measurement error of the instrument is the same as the measurement error of the confounder.\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "The presence of the black hole is enforced using the notion of\n",
      "apparent curvature, which is defined as the curvature of the\n",
      "spacetimes with respect to the black hole.\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "It has been recently argued that\n",
      "the post-merger ringdown waveform of exotic ultracompact objects is initially\n",
      "identical to that of a black-hole, and that putative corrections at the horizon\n",
      "scale will appear as secondary black-hole waveforms.\n",
      "19\n",
      "Similarity to how the\n",
      "elastic net generalizes lasso and ridge regression to gradient descent and\n",
      "\n",
      "forward stagewise regression is shown in Figure 2.\n",
      "20\n",
      "We discuss two key\n",
      "principles for the design of gateway nodes and scalable gateway protocols,\n",
      "namely (i) the opaque ledgers principle as the analogue of the autonomous\n",
      "systems principle in IP datagrams, and (ii) the transparent ledgers\n",
      "principles as the analogue of the autonomous systems principle in IP datagrams.\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "These attacks are becoming more and more sophisticated, and\n",
      "more and more hospitals are being targeted by ransomware attacks.\n",
      "25\n",
      "26\n",
      "In particular we give exact expressions, in the form of integral transforms,\n",
      "for the black hole entropy of a black hole in a quantum field theory.\n",
      "27\n",
      "Typical localisation methods make use of the time of day of the day of the day of the day of the day of the day of the day of the day of the day of the day of the day of the day of the day of the day of the day of the day of the day of the day of the day of the day of the day of the day of the day of the day of the\n",
      "28\n",
      "29\n",
      "In this paper, we demonstrate how signal estimation can be operated\n",
      "directly through such quadratic sketches--equivalent to the quadratic\n",
      "sketching of a quadratic matrix.\n",
      "30\n",
      "31\n",
      "The sequential updating and greedy search (SUGS) algorithm (Wang and\n",
      "Dunson, 2011) was proposed as a fast method for performing approximate Bayesian\n",
      "inference in DP mixture models (Wang and Dunson, 2011).\n",
      "32\n",
      "Recent studies have focused on estimating the health effects of a large number\n",
      "of environmental exposures, or \"latent\" exposures.\n",
      "33\n",
      "In this paper, we\n",
      "present a novel approach to fingerprinting Wi-Fi chipsets, based on the use of a unique\n",
      "fingerprint that can be used as an additional means of authentication to enhance security.\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "One widely used approach for\n",
      "dealing with the signal reconstruction problem is to reconstruct a time-domain\n",
      "spectrogram from a time-domain signal.\n",
      "39\n",
      "Being stabilized to a high-finesse cavity with a\n",
      "fractional frequency stability of $3.8\\times10^{-15}$ at 0.1 s, the comb-rooted\n",
      "synthesizer produces multiple optical frequencies of ultra-narrow-bandwidth.\n",
      "40\n",
      "Indeed,\n",
      "statistical \"significance\" is evidence for the presence or absence of a hypothesis, not\n",
      "the presence or absence of the hypothesis itself.\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "We observe strong-coupling\n",
      "of the excitonic dipoles in quasi 2-dimensional CdSe/CdxZnS1-xS nanoplatelets\n",
      "with respect to the CdSe/CdxZnS1-xS nanoplatelets on the Au substrate.\n",
      "45\n",
      "46\n",
      "The approach is analogous to the mean-variance\n",
      "parameterization of a linear regression model, except that the covariance matrix is\n",
      "parameterized as a parsimonious quadratic function of explanatory\n",
      "variables rather than as a linear quadratic function of explanatory\n",
      "variables.\n",
      "47\n",
      "In this context, we\n",
      "created three different types of filter banks: (1) a single-channel filter bank,\n",
      "(2) a two-channel filter bank, and (3) a three-channel filter bank.\n",
      "48\n",
      "49\n",
      "Across\n",
      "an empirical set of cases, we find that the average treatment effect estimate\n",
      "is significantly larger than the mean treatment effect estimate in cases with missing\n",
      "assumptions.\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "The bootstrap has shown promise in solving this problem, but empirical\n",
      "evidence often indicates that some bootstrap methods have difficulty in\n",
      "maintaining the correct coverage probability, while others do not.\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "In this work, we consider a particular\n",
      "counter-example to the `no-hair' conjecture in which the black hole is a\n",
      "spherically symmetric black hole.\n",
      "58\n",
      "For these studies, when\n",
      "included in a meta-analysis, it is recommended to use the five-number\n",
      "summary (including the sample median, the first and third quartiles, and the minimum and maximum\n",
      "\n",
      "values) rather than the sample mean and standard deviation, particularly for skewed data.\n",
      "59\n",
      "60\n",
      "61\n",
      "Accelerometers for these applications are very expensive and\n",
      "difficult to obtain, so it is very important to have accurate acceleration measurements down\n",
      "to ever lower frequencies and with ever lower noise.\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "Especially, recent works have shown that\n",
      "NTT is the most efficient method for multiplying polynomials of\n",
      "high degree with integer coefficients, due to its series ofadvantages in terms of algorithm and implementation, and is consequentlywidely-used and particularly fundamental in the practical implementations oflattice-based cryptographic schemes.\n",
      "67\n",
      "68\n",
      "Our novel M-CiSSA approach is based on the assumption that the\n",
      "sources of variation of a set of time series are independent of each other, and that the\n",
      "sources of variation of a set of time series are independent of each other, and that the\n",
      "sources of variation of a set of time series are independent of\n",
      "69\n",
      "70\n",
      "71\n",
      "As of today, WSNs are pretty much used\n",
      "by any monitoring system: from those that are health care related, to those that are\n",
      "automotive related, to those that are weather related, to those that are\n",
      "automotive related, to those that are environmental related, to those that are\n",
      "automotive related, to those that are weather related, to those that are environmental related, to those that are\n",
      "automotive related, to those that are weather related,\n",
      "72\n",
      "A SWG structure composed\n",
      "of a single dielectric layer with the appropriate patterning can sometimes\n",
      "perform as well as thirty or forty dielectric DBR layers, while providing new\n",
      "functionalities such as low-noise, high-reflection, high-bandwidth, and\n",
      "low-noise-to-bandwidth (LBNW) applications.\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "<cs.CR>:   The BBCRS scheme is a variant of the McEliece public-key encryption scheme\n",
      "where the hiding phase is performed by taking the inverse of a matrix which is\n",
      "of the form $\\mathbf{T} +\\mathbf{R}$ where $\\mathbf{T}$ is a sparse matrix with\n",
      "average row/column weight equal to a very small quantity $m$, usually less than $1$\n",
      "and $\\mathbf{R}$ is a sparse matrix with\n",
      "average row/column weight equal to a very small quantity $m$, usually less than $1$\n",
      "where $\\mathbf{R}$ is a sparse matrix with\n",
      "where $\\mathbf{T}$\n",
      "78\n",
      "In this paper, we propose to approximate the\n",
      "log-likelihood with subsamples taken according to nonuniform subsampling\n",
      "probabilities, and derive the most likely optimal (MLO) subsampling\n",
      "probabilities for MCMC.\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "To address those issues, we propose three\n",
      "automated approaches to detect slowing in EEG: Threshold-based detection,\n",
      "automated threshold-based detection, and automated threshold-based\n",
      "detection.\n",
      "91\n",
      "92\n",
      "In this paper, we propose a joint pilot design and\n",
      "channel estimation based on deep residual learning in order to mitigate the\n",
      "effects of pilot contamination and the effects of pilot contamination become more severe due to hardware impairments.\n",
      "93\n",
      "By considering the\n",
      "conformal symmetry as exact at the level of the Lagrangian and broken in the\n",
      "vacuum, a consistent model is found with an invariant Lagrangian.\n",
      "94\n",
      "95\n",
      "The doubly-resonant OPO oscillates\n",
      "simultaneously at a Stokes and an anti Stokes wavelength shift of >50 nm from\n",
      "the pump wavelength that lies at the resonant frequency of the chalcogenide microwire.\n",
      "96\n",
      "From another\n",
      "side, such dark fluid models may describe the transition from a\n",
      "phantom era to a superacceleration era, and from a superacceleration\n",
      "era to a non-phantom era, and from a non-phantom era to a phantom era, and from a phantom era to a non-phantom era, and from a phantom era to a\n",
      "97\n",
      "In FD-NOMA systems power allocation\n",
      "is a very computationally intense non-convex problem due to the presence of\n",
      "strong interference and the integrality condition on channel allocation.\n",
      "98\n",
      "99\n",
      "In this study, we fabricate and fabricate a\n",
      "photonic crystal with a refractive index of 0.5 and a diffractive index of 0.5.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "flags = [1000, 4000, 8000, 15000, 27000]\n",
    "num_of_repetitive_samples = 0\n",
    "\n",
    "for i in range(len(df)):\n",
    "    \n",
    "    if i in flags:\n",
    "        print(i)\n",
    "\n",
    "    \n",
    "    # You can change the length to test different values.    \n",
    "    prompt = df['text'][i].split(\" \")[:50]\n",
    "    prompt = ' '.join(prompt)\n",
    "    init_sentences = nltk.sent_tokenize(prompt)\n",
    "    length = len(init_sentences)\n",
    "    input_ids = tokenizer(prompt, return_tensors='pt').to(device)\n",
    "    \n",
    "    # You can modify the max_length based on the prompt length.\n",
    "    temp_text = model.generate(**input_ids, max_length=150, num_beams = 6)\n",
    "    temp_text = tokenizer.decode(temp_text[0], skip_special_tokens=True)\n",
    "    \n",
    "   \n",
    "    sentences = nltk.sent_tokenize(temp_text)\n",
    "    final_text = sentences[length-1]\n",
    "    \n",
    "    if has_repetition(final_text):\n",
    "        #print(final_text)\n",
    "        num_of_repetitive_samples += 1\n",
    "        continue\n",
    "    \n",
    "    orig_sentences = nltk.sent_tokenize(df['text'][i])\n",
    "    score = similarity_score(orig_sentences[length-1], final_text)\n",
    "    diff = len(final_text.split(' ')) - len(init_sentences[length-1].split(\" \"))\n",
    "    \n",
    "    \n",
    "    results[str(diff)][0] += 1\n",
    "    results[str(diff)][1] += score[0].item()\n",
    "    #print(\"Score: \" + str(score))\n",
    "    #print(\"Diff: \" + str(diff))\n",
    "    #print(\"------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "52b29e83-24c1-4999-a758-d0847dba2da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(results)):\n",
    "    if results[str(i)][0] > 0:\n",
    "        results[str(i)][1] = results[str(i)][1]/results[str(i)][0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "98c7169c-5d30-444f-82be-6f46d87a0ed4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'0': [5, 0.8857958436012268], '1': [3, 0.9013360540072123], '2': [6, 0.722661112745603], '3': [4, 0.6346058398485184], '4': [7, 0.775366987500872], '5': [2, 0.6704206466674805], '6': [2, 0.48442715406417847], '7': [6, 0.803336888551712], '8': [4, 0.5602045580744743], '9': [0, 0], '10': [1, 0.8796998262405396], '11': [2, 0.6954313218593597], '12': [0, 0], '13': [3, 0.41146397590637207], '14': [2, 0.5290040820837021], '15': [3, 0.46533167362213135], '16': [2, 0.370731383562088], '17': [4, 0.5017414093017578], '18': [1, 0.36304786801338196], '19': [2, 0.4280588924884796], '20': [1, 0.3395509123802185], '21': [1, 0.33076173067092896], '22': [1, 0.6550519466400146], '23': [0, 0], '24': [0, 0], '25': [0, 0], '26': [0, 0], '27': [0, 0], '28': [0, 0], '29': [0, 0], '30': [0, 0], '31': [0, 0], '32': [0, 0], '33': [0, 0], '34': [0, 0], '35': [0, 0], '36': [0, 0], '37': [0, 0], '38': [0, 0], '39': [0, 0], '40': [0, 0], '41': [0, 0], '42': [0, 0], '43': [0, 0], '44': [0, 0], '45': [0, 0], '46': [0, 0], '47': [0, 0], '48': [0, 0], '49': [0, 0], '50': [0, 0], '51': [0, 0], '52': [0, 0], '53': [0, 0], '54': [0, 0], '55': [0, 0], '56': [0, 0], '57': [0, 0], '58': [0, 0], '59': [0, 0], '60': [0, 0], '61': [0, 0], '62': [0, 0], '63': [0, 0], '64': [0, 0], '65': [0, 0], '66': [0, 0], '67': [0, 0], '68': [0, 0], '69': [0, 0], '70': [0, 0], '71': [0, 0], '72': [0, 0], '73': [0, 0], '74': [0, 0], '75': [0, 0], '76': [0, 0], '77': [0, 0], '78': [0, 0], '79': [0, 0], '80': [0, 0], '81': [0, 0], '82': [0, 0], '83': [0, 0], '84': [0, 0], '85': [0, 0], '86': [0, 0], '87': [0, 0], '88': [0, 0], '89': [0, 0], '90': [0, 0], '91': [0, 0], '92': [0, 0], '93': [0, 0], '94': [0, 0], '95': [0, 0], '96': [0, 0], '97': [0, 0], '98': [0, 0], '99': [0, 0]}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3701ad0b-9e37-47ae-9ccd-b08811d7c30f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results = pd.DataFrame.from_dict(results, orient='index', columns=['Number', 'Mean'])\n",
    "\n",
    "# Set the name of saved file in the same format of the fine-tuned model name\n",
    "df_results.to_csv('test.csv', index_label='key')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19e67e3-7950-45c3-8fcc-d3f07f80f714",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (anaconda3-test)",
   "language": "python",
   "name": "conda-env-anaconda3-test-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
