{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57feed3f-800f-47e2-a331-a278221ca27a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import the libraries\n",
    "\n",
    "import nltk\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513360fe-4f09-4174-b568-f1f9691aa57d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device(\"cuda\")\n",
    "# device = \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ce9e51-d5de-4c05-a7f9-0a384237515f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('punkt')\n",
    "model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d761516d-8384-4beb-9e89-74104d4cc220",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'gpt2-large'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af78b9a-3070-4720-8018-a5a655c16fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function calculates the cosine similarity between two text inputs, using a pre-trained model to generate embeddings and returning a score from -1 to 1.\n",
    "def similarity_score(text1, text2):\n",
    "    embeddings1 = model.encode(text1, convert_to_tensor=True)\n",
    "    embeddings2 = model.encode(text2, convert_to_tensor=True)\n",
    "    return util.cos_sim(embeddings1, embeddings2)\n",
    "\n",
    "# This function calculates the similarity using the BERT model given the original and generated texts\n",
    "def approximated_matching(original_text, generated_text, original_tokens, prompt_length):\n",
    "    generated_sentences = nltk.sent_tokenize(generated_text)\n",
    "    # Convert the tokens back into text\n",
    "    original_sub_string = tokenizer.decode(original_tokens[\"input_ids\"][0][:prompt_length])\n",
    "    \n",
    "    # Get the sentences of sub-strings\n",
    "    original_sentences_length = len(nltk.sent_tokenize(original_sub_string))\n",
    "    original_sentence = nltk.sent_tokenize(original_text)[original_sentences_length-1]\n",
    "    generated_sentence = generated_sentences[original_sentences_length-1]\n",
    " \n",
    "    score = similarity_score(original_sentence, generated_sentence)\n",
    "    \n",
    "    # Calculating the length difference\n",
    "    temp_text = ' '.join(generated_sentences[:original_sentences_length])\n",
    "    temp_text = tokenizer(temp_text, return_tensors=\"pt\")\n",
    "    \n",
    "    diff = len(temp_text[\"input_ids\"][0]) - prompt_length\n",
    "\n",
    "    return score, diff, original_sentence, generated_sentence\n",
    "\n",
    "def exact_matching(original_tokens, generated_tokens, prompt_length):\n",
    "    pointer = 0\n",
    "    while original_tokens['input_ids'][0][prompt_length + pointer] == generated_tokens['input_ids'][0][prompt_length + pointer]:\n",
    "        pointer += 1\n",
    "    return pointer\n",
    "\n",
    "# If the decoding algorithm is one of the greedy search or beam search, the generated text might be repetitive.\n",
    "# Hence, we wrote this function to check if the generated text is repetitive. If so, we will remove that sample.\n",
    "# This function determines if there is repetitive text in a generated text using a sliding window of the specified size.\n",
    "# Returns True if there is any repetition and False otherwise.\n",
    "def has_repetition(generated_text, window_size=10):\n",
    "    window_start = 0\n",
    "    window_end = window_size\n",
    "    while window_end < len(generated_text):\n",
    "        window_text = generated_text[window_start:window_end]\n",
    "        if window_text in generated_text[window_end:]:\n",
    "            return True\n",
    "        window_start += 1\n",
    "        window_end += 1\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7049c3-4c30-46d0-a41d-9f53b67dca6e",
   "metadata": {},
   "source": [
    "This cell reads in pre-generated text for several models, and calculates the similarity between the generated text and a set of original texts. It skips any text that does not meet certain criteria and outputs the average similarity score for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd79365-9a27-419b-b581-a425d88d53aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_text_path = \"./generated_text_2/\"\n",
    "generated_text_files = [f for f in os.listdir(generated_text_path) if f.endswith(\".csv\")]\n",
    "prompt_length = 70\n",
    "\n",
    "for generated_text_file in generated_text_files:\n",
    "    print(f\"---------------------------------\")\n",
    "    print(f\"Processing {generated_text_file}\")\n",
    "\n",
    "    generated_df = pd.read_csv(f\"{generated_text_path}/{generated_text_file}\")\n",
    "    similarity_scores = []\n",
    "    length_diffs = []\n",
    "    exact_matches = []\n",
    "    original_sentences = []\n",
    "    generated_sentences = []\n",
    "    repetitions = []\n",
    "    \n",
    "    for (original_text, generated_text) in tqdm(zip(generated_df['text'], generated_df['promptLength70_numBeams1']), total=len(generated_df.index)):\n",
    "        original_tokens = tokenizer(original_text, return_tensors=\"pt\").to(device)\n",
    "        generated_tokens = tokenizer(generated_text, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "        sim_score, length_diff, original_sentence, generated_sentence = approximated_matching(original_text, generated_text, original_tokens, prompt_length)\n",
    "        exact_match = exact_matching(original_tokens, generated_tokens, prompt_length)\n",
    "        has_rep = has_repetition(generated_sentence)\n",
    "                \n",
    "        similarity_scores.append(sim_score.cpu().item())\n",
    "        exact_matches.append(exact_match)\n",
    "        length_diffs.append(length_diff)\n",
    "        original_sentences.append(original_sentence)\n",
    "        generated_sentences.append(generated_sentence)\n",
    "        repetitions.append(has_rep)\n",
    "        \n",
    "    generated_df['similarity_score'] = similarity_scores\n",
    "    generated_df['exact_match'] = exact_matches\n",
    "    generated_df['length_diff'] = length_diffs\n",
    "    generated_df['original_sentence'] = original_sentences\n",
    "    generated_df['generated_sentence'] = generated_sentences\n",
    "    generated_df['has_repetition'] = repetitions\n",
    "        \n",
    "    generated_df.to_csv(f\"./metrics/metrics_{generated_text_file}\", index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Finetune [conda env:Memorization-finetune]",
   "language": "python",
   "name": "conda-env-Memorization-finetune-finetune"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
