{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13f0899-8901-469b-8de8-dc7a1684315c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForCausalLM, AutoConfig, AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0055baa1-afcc-497c-bbf6-38fea95788da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODIFY THE PARAMS HERE\n",
    "model_type = \"gpt2-large\"\n",
    "model_path = \"./models/gpt2-large/arxiv_50000/blk0.7/checkpoint-47104/pytorch_model.bin\"\n",
    "data_file = \"arxiv_5000\" # Please put data_file (.csv) in ./data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a9ec2b-3fa2-4db9-859b-0b7754c1fbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_type)\n",
    "config = AutoConfig.from_pretrained(model_type)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_type, config=config)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# Prepare for batch generation\n",
    "tokenizer.padding_side = \"left\"\n",
    "tokenizer.truncation_side = \"right\"\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9220545c-1023-4948-8cbb-fe397ca2a69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(model_path))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182f67e7-b965-46bc-a89f-faeb2e9973fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_datasets = load_dataset('csv', data_files={'train': \"./data/{}.csv\".format(data_file)}, split=f\"train[15%:]\")\n",
    "    \n",
    "column_names = list(raw_datasets.features)\n",
    "text_column_name = \"text\" if \"text\" in column_names else column_names[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b6c8b7-9192-46d6-b779-72083688f201",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_prompt(examples, max_length=50):    \n",
    "    return tokenizer(\n",
    "        examples[text_column_name], return_tensors='pt',\n",
    "        padding=True, truncation=True, max_length=max_length,\n",
    "    )\n",
    "\n",
    "def generate_text(data, batch_size=100, prompt_length=50, max_new_tokens=100, num_beams=1):\n",
    "    # Create batches of tokenized prompts\n",
    "    prompts = data.map(\n",
    "        tokenize_prompt,\n",
    "        batched=True,\n",
    "        batch_size=batch_size,\n",
    "        remove_columns=column_names,\n",
    "        desc=\"Running tokenizer on dataset\",\n",
    "        fn_kwargs={\"max_length\": prompt_length},\n",
    "    )\n",
    "    prompts.set_format(type=\"torch\")\n",
    "    \n",
    "    # Map the generation function over each batch\n",
    "    def model_gen(examples):\n",
    "        return {\"model_gen\": model.generate(\n",
    "            input_ids=examples[\"input_ids\"].to(device),\n",
    "            attention_mask=examples[\"attention_mask\"].to(device),\n",
    "            max_new_tokens=max_new_tokens,\n",
    "            num_beams=num_beams,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "        )}\n",
    "    \n",
    "    model_outputs = prompts.map(\n",
    "        model_gen,\n",
    "        batched=True,\n",
    "        batch_size=batch_size,\n",
    "        desc=\"Generating\",\n",
    "    )\n",
    "    \n",
    "    # Decode the tokenized outputs\n",
    "    return tokenizer.batch_decode(model_outputs[\"model_gen\"], skip_special_tokens=True)\n",
    "\n",
    "def run_and_save_gen(prompt_lengths, num_beams):\n",
    "    output_df = pd.DataFrame({\"text\": raw_datasets[\"text\"]})\n",
    "    \n",
    "    for prompt_length in prompt_lengths:\n",
    "        for beams in num_beams:\n",
    "            config = {\n",
    "                \"batch_size\": 100, # Larger is faster, but costs more RAM\n",
    "                \"prompt_length\": prompt_length, # Number of tokens, NOT words\n",
    "                \"max_new_tokens\": 100,\n",
    "                \"num_beams\": beams, # 1 for greedy search\n",
    "            }\n",
    "            print(config)\n",
    "\n",
    "            outputs = generate_text(raw_datasets, **config)\n",
    "            \n",
    "            col_name = \"promptLength{}_numBeams{}\".format(prompt_length, beams)\n",
    "            output_df[col_name] = outputs\n",
    "\n",
    "    output_path = \"./model_gen_{}_{}.csv\".format(model_type, data_file)\n",
    "    print(\"Saving generation output to\", output_path)\n",
    "\n",
    "    output_df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1497ccb2-1196-4046-8990-6979552bee22",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_and_save_gen(\n",
    "    prompt_lengths=[50, 100, 200, 500],\n",
    "    num_beams=[1, 100],\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Finetune (Memorization-finetune)",
   "language": "python",
   "name": "conda-env-Memorization-finetune-finetune"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
