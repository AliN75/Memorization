{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1babc32c-f888-4e09-88a7-9888d89e1502",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/anaseh_umass_edu/anaconda3/envs/test/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "# Copyright 2020 The HuggingFace Inc. team. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "\"\"\"\n",
    "Fine-tuning the library models for causal language modeling (GPT, GPT-2, CTRL, ...) on a text file or a dataset.\n",
    "Here is the full list of checkpoints on the hub that can be fine-tuned by this script:\n",
    "https://huggingface.co/models?filter=text-generation\n",
    "\"\"\"\n",
    "# You can also adapt this script on your own causal language modeling task. Pointers for this are left as comments.\n",
    "\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import sys\n",
    "from dataclasses import dataclass, field\n",
    "from itertools import chain\n",
    "from typing import Optional\n",
    "import pandas as pd\n",
    "\n",
    "import datasets\n",
    "import evaluate\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from accelerate import Accelerator, DistributedType\n",
    "\n",
    "import transformers\n",
    "from transformers import (\n",
    "    CONFIG_MAPPING,\n",
    "    MODEL_FOR_CAUSAL_LM_MAPPING,\n",
    "    AutoConfig,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoTokenizer,\n",
    "    HfArgumentParser,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    "    is_torch_tpu_available,\n",
    "    set_seed,\n",
    ")\n",
    "from transformers.testing_utils import CaptureLogger\n",
    "from transformers.trainer_utils import get_last_checkpoint\n",
    "from transformers.utils import check_min_version, send_example_telemetry\n",
    "from transformers.utils.versions import require_version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d7fd762-c11a-45a4-865e-5e43b7155636",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Will error if the minimal version of Transformers is not installed. Remove at your own risks.\n",
    "\n",
    "\n",
    "require_version(\"datasets>=1.8.0\", \"To fix: pip install -r examples/pytorch/language-modeling/requirements.txt\")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "037b6582-df93-4114-a2d7-0b27786a4c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sending telemetry. Tracking the example usage helps us better allocate resources to maintain them. The\n",
    "# information sent is the one passed as arguments along with your Python/PyTorch versions.\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012a24a6-210b-43cb-9020-64e51c0d118d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is the directory where the model will be saved. Follow this format to set the directory: './size_of_the_model (e.g. small) / type_of_the_model (e.g. gpt2) / dataset_size (e.g. 50000) / number_of_epochs (e.g. 5)\n",
    "# Change the direction appropriately\n",
    "direction = './' + 'gpt_2_test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac1c04a3-5ce7-4219-aa75-503eabc92433",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# You might need to change the num_train_epochs or save_steps. After 'save_steps' number of steps, a version of the model will be saved as a checkpoint. To save memory, try to choose the best value for this argument.\n",
    "training_args = TrainingArguments(output_dir=direction,\n",
    "                                  num_train_epochs=5,\n",
    "                                  logging_steps=5000,\n",
    "                                  save_strategy='epoch',                                   \n",
    "                                  per_device_train_batch_size=2,\n",
    "                                  per_device_eval_batch_size=2,\n",
    "                                  warmup_steps=100,\n",
    "                                  weight_decay=0.01,  \n",
    "                                  logging_dir='./logs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7d9865d-ca08-4e3f-9ba6-70e6818d5413",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/20/2023 12:50:46 - WARNING - datasets.builder - Using custom data configuration default-70b08638e51d0fe0\n",
      "Downloading and preparing dataset csv/default to /home/anaseh_umass_edu/.cache/huggingface/datasets/csv/default-70b08638e51d0fe0/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data files: 100%|██████████| 1/1 [00:00<00:00, 1829.18it/s]\n",
      "Extracting data files: 100%|██████████| 1/1 [00:00<00:00, 104.91it/s]\n",
      "                            \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset csv downloaded and prepared to /home/anaseh_umass_edu/.cache/huggingface/datasets/csv/default-70b08638e51d0fe0/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 97.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/20/2023 12:50:47 - WARNING - datasets.builder - Using custom data configuration default-70b08638e51d0fe0\n",
      "03/20/2023 12:50:47 - WARNING - datasets.builder - Reusing dataset csv (/home/anaseh_umass_edu/.cache/huggingface/datasets/csv/default-70b08638e51d0fe0/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n",
      "03/20/2023 12:50:47 - WARNING - datasets.builder - Using custom data configuration default-70b08638e51d0fe0\n",
      "03/20/2023 12:50:47 - WARNING - datasets.builder - Reusing dataset csv (/home/anaseh_umass_edu/.cache/huggingface/datasets/csv/default-70b08638e51d0fe0/0.0.0/51cce309a08df9c4d82ffd9363bbe090bf173197fc01a71b034e8594995a1a58)\n"
     ]
    }
   ],
   "source": [
    "# Set seed before initializing model.\n",
    "set_seed(1234)\n",
    "\n",
    "# Get the datasets: you can either provide your own CSV/JSON/TXT training and evaluation files (see below)\n",
    "# or just provide the name of one of the public datasets available on the hub at https://huggingface.co/datasets/\n",
    "# (the dataset will be downloaded automatically from the datasets Hub).\n",
    "#\n",
    "# For CSV/JSON files, this script will use the column called 'text' or the first column if no column called\n",
    "# 'text' is found. You can easily tweak this behavior (see below).\n",
    "#\n",
    "# In distributed training, the load_dataset function guarantee that only one local process can concurrently\n",
    "# download the dataset.\n",
    "\n",
    "data_files = {}\n",
    "dataset_args = {}\n",
    "\n",
    "\n",
    "#Change the file name if needed.\n",
    "data_files[\"train\"] = 'train.csv'\n",
    "\n",
    "raw_datasets = load_dataset('csv',\n",
    "    data_files=data_files,\n",
    ")\n",
    "# If no validation data is there, validation_split_percentage will be used to divide the dataset.\n",
    "if \"validation\" not in raw_datasets.keys():\n",
    "    raw_datasets[\"validation\"] = load_dataset('csv',\n",
    "        data_files=data_files,\n",
    "        split=f\"train[:15%]\",\n",
    "    )\n",
    "    raw_datasets[\"train\"] = load_dataset('csv',\n",
    "        data_files=data_files,\n",
    "        split=f\"train[15%:]\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a2481a4-8815-4940-bf1b-7276239b8165",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See more about loading any type of standard or custom dataset (from files, python dict, pandas DataFrame, etc) at\n",
    "# https://huggingface.co/docs/datasets/loading_datasets.html.\n",
    "\n",
    "# Load pretrained model and tokenizer\n",
    "#\n",
    "# Distributed training:\n",
    "# The .from_pretrained methods guarantee that only one local process can concurrently\n",
    "# download model & vocab.\n",
    "\n",
    "#config_kwargs = {\n",
    "#    \"cache_dir\": model_args.cache_dir,\n",
    "#    \"revision\": model_args.model_revision,\n",
    "#    \"use_auth_token\": True if model_args.use_auth_token else None,\n",
    "#}\n",
    "\n",
    "\n",
    "\n",
    "# Change the config if needed.\n",
    "config = AutoConfig.from_pretrained(\"gpt2-large\")\n",
    "\n",
    "\n",
    "# Change the tokenizer if needed.\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f856fab6-5c39-47ec-a4b1-901d7ba3a38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the model if needed.\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2-large\", config=config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "750a85ba-08c6-4907-a1a5-f3f8bad6d6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We resize the embeddings only when necessary to avoid index errors. If you are creating a model from scratch\n",
    "# on a small vocab and want a smaller embedding size, remove this test.\n",
    "embedding_size = model.get_input_embeddings().weight.shape[0]\n",
    "if len(tokenizer) > embedding_size:\n",
    "    model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d648610c-2ad1-447b-b68c-9ae121b6d812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell only if you want to fine-tune a part of the model\n",
    "# Freezing layers\n",
    "layers = [5, 10, 15]\n",
    "\n",
    "# First freeze the whole model\n",
    "for params in model.parameters():\n",
    "    params.requires_grad = False\n",
    "\n",
    "# Then unfreeze the selected blocks\n",
    "for layer in layers:    \n",
    "    for params in model.transformer.h[layer].parameters():\n",
    "        params.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db444a8-5ce8-47d6-8c4b-91425cfb3578",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To make sure if selected blocks are frozen\n",
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(name, param.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6802be4c-69cd-463a-bb13-785d560309ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Preprocessing the datasets.\n",
    "# First we tokenize all the texts.\n",
    "#if training_args.do_train:\n",
    "column_names = list(raw_datasets[\"train\"].features)\n",
    "#else:\n",
    "#    column_names = list(raw_datasets[\"validation\"].features)\n",
    "text_column_name = \"text\" if \"text\" in column_names else column_names[0]\n",
    "\n",
    "\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    \n",
    "    output = tokenizer(examples[text_column_name])\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "09b61e1b-8382-47b0-8462-6719555cb0e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/20/2023 12:51:41 - WARNING - datasets.fingerprint - Parameter 'function'=<function tokenize_function at 0x7f983f053130> of the transform datasets.arrow_dataset.Dataset._map_single couldn't be hashed properly, a random hash was used instead. Make sure your transforms and parameters are serializable with pickle or dill for the dataset fingerprinting and caching to work. If you reuse this transform, the caching mechanism will consider it to be different from the previous calls and recompute everything. This warning is only showed once. Subsequent hashing failures won't be showed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running tokenizer on dataset: 100%|██████████| 1/1 [00:00<00:00,  4.77ba/s]\n",
      "Running tokenizer on dataset: 100%|██████████| 1/1 [00:00<00:00,  1.43ba/s]\n"
     ]
    }
   ],
   "source": [
    "with training_args.main_process_first():\n",
    "        tokenized_datasets = raw_datasets.map(\n",
    "            tokenize_function,\n",
    "            batched=True,\n",
    "            remove_columns=column_names,\n",
    "            desc=\"Running tokenizer on dataset\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6ac28ac5-33ff-4de7-9cc6-dbf98aa8f225",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "03/20/2023 12:51:47 - WARNING - __main__ - The chosen tokenizer supports a `model_max_length` that is longer than the default `block_size` value of 1024. If you would like to use a longer `block_size` up to `tokenizer.model_max_length` you can override this default with `--block_size xxx`.\n"
     ]
    }
   ],
   "source": [
    "block_size = tokenizer.model_max_length\n",
    "if block_size > 1024:\n",
    "    logger.warning(\n",
    "        \"The chosen tokenizer supports a `model_max_length` that is longer than the default `block_size` value\"\n",
    "        \" of 1024. If you would like to use a longer `block_size` up to `tokenizer.model_max_length` you can\"\n",
    "        \" override this default with `--block_size xxx`.\"\n",
    "    )\n",
    "    block_size = 1024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb9656ad-7a76-49a8-bb3c-c4b46aff9db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main data processing function that will concatenate all texts from our dataset and generate chunks of block_size.\n",
    "def group_texts(examples):\n",
    "    # Concatenate all texts.\n",
    "    concatenated_examples = {k: list(chain(*examples[k])) for k in examples.keys()}\n",
    "    total_length = len(concatenated_examples[list(examples.keys())[0]])\n",
    "    # We drop the small remainder, we could add padding if the model supported it instead of this drop, you can\n",
    "    # customize this part to your needs.\n",
    "    if total_length >= block_size:\n",
    "        total_length = (total_length // block_size) * block_size\n",
    "    # Split by chunks of max_len.\n",
    "    result = {\n",
    "        k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n",
    "        for k, t in concatenated_examples.items()\n",
    "    }\n",
    "    result[\"labels\"] = result[\"input_ids\"].copy()\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1906ffa7-c84b-4ce4-8c8c-64de50e42a9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Grouping texts in chunks of 1024: 100%|██████████| 1/1 [00:00<00:00, 13.88ba/s]\n",
      "Grouping texts in chunks of 1024: 100%|██████████| 1/1 [00:00<00:00,  3.20ba/s]\n"
     ]
    }
   ],
   "source": [
    "# Note that with `batched=True`, this map processes 1,000 texts together, so group_texts throws away a remainder\n",
    "# for each of those groups of 1,000 texts. You can adjust that batch_size here but a higher value might be slower\n",
    "# to preprocess.\n",
    "#\n",
    "# To speed up this part, we use multiprocessing. See the documentation of the map method for more information:\n",
    "# https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.map\n",
    "\n",
    "with training_args.main_process_first():\n",
    "        lm_datasets = tokenized_datasets.map(\n",
    "            group_texts,\n",
    "            batched=True,\n",
    "            desc=f\"Grouping texts in chunks of {block_size}\",\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d52e0bad-3382-419c-a080-26a191126953",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = lm_datasets[\"train\"]\n",
    "max_train_samples = len(train_dataset)\n",
    "train_dataset = train_dataset.select(range(max_train_samples))\n",
    "\n",
    "\n",
    "eval_dataset = lm_datasets[\"validation\"]\n",
    "\n",
    "max_eval_samples = len(eval_dataset)\n",
    "eval_dataset = eval_dataset.select(range(max_eval_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b523ed2e-1376-477e-9134-f4841d616ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_logits_for_metrics(logits, labels):\n",
    "    if isinstance(logits, tuple):\n",
    "        # Depending on the model and config, logits may contain extra tensors,\n",
    "        # like past_key_values, but logits always come first\n",
    "        logits = logits[0]\n",
    "    return logits.argmax(dim=-1)\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1970faea-e9cf-4675-8882-4d15a0b06550",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    # preds have the same shape as the labels, after the argmax(-1) has been calculated\n",
    "    # by preprocess_logits_for_metrics but we need to shift the labels\n",
    "    labels = labels[:, 1:].reshape(-1)\n",
    "    preds = preds[:, :-1].reshape(-1)\n",
    "    return metric.compute(predictions=preds, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "599477fd-f1ce-4f1b-a2a3-9d544d214693",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_checkpoint = None\n",
    "if os.path.isdir(training_args.output_dir) and training_args.do_train and not training_args.overwrite_output_dir:\n",
    "    last_checkpoint = get_last_checkpoint(training_args.output_dir)\n",
    "    if last_checkpoint is None and len(os.listdir(training_args.output_dir)) > 0:\n",
    "        raise ValueError(\n",
    "            f\"Output directory ({training_args.output_dir}) already exists and is not empty. \"\n",
    "            \"Use --overwrite_output_dir to overcome.\"\n",
    "        )\n",
    "    elif last_checkpoint is not None and training_args.resume_from_checkpoint is None:\n",
    "        logger.info(\n",
    "            f\"Checkpoint detected, resuming training at {last_checkpoint}. To avoid this behavior, change \"\n",
    "            \"the `--output_dir` or add `--overwrite_output_dir` to train from scratch.\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d8272274-bb49-4ce8-b727-ce996edeb89e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize our Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    # Data collator will default to DataCollatorWithPadding, so we change it.\n",
    "    data_collator=default_data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    "    preprocess_logits_for_metrics=preprocess_logits_for_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33cd0527-e832-4c72-8681-e1450f971ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/anaseh_umass_edu/anaconda3/envs/test/lib/python3.10/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 41\n",
      "  Num Epochs = 5\n",
      "  Instantaneous batch size per device = 2\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 2\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 105\n",
      "  Number of trainable parameters = 1315575808\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='105' max='105' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [105/105 06:07, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./gpt_2_test/checkpoint-100\n",
      "Configuration saved in ./gpt_2_test/checkpoint-100/config.json\n",
      "Configuration saved in ./gpt_2_test/checkpoint-100/generation_config.json\n",
      "Model weights saved in ./gpt_2_test/checkpoint-100/pytorch_model.bin\n",
      "tokenizer config file saved in ./gpt_2_test/checkpoint-100/tokenizer_config.json\n",
      "Special tokens file saved in ./gpt_2_test/checkpoint-100/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Saving model checkpoint to ./gpt_2_test\n",
      "Configuration saved in ./gpt_2_test/config.json\n",
      "Configuration saved in ./gpt_2_test/generation_config.json\n",
      "Model weights saved in ./gpt_2_test/pytorch_model.bin\n",
      "tokenizer config file saved in ./gpt_2_test/tokenizer_config.json\n",
      "Special tokens file saved in ./gpt_2_test/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "checkpoint = None\n",
    "if training_args.resume_from_checkpoint is not None:\n",
    "    checkpoint = training_args.resume_from_checkpoint\n",
    "elif last_checkpoint is not None:\n",
    "    checkpoint = last_checkpoint\n",
    "train_result = trainer.train(resume_from_checkpoint=checkpoint)\n",
    "trainer.save_model()  # Saves the tokenizer too for easy upload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "622a0747-fa66-4e69-85a5-c14ab7b452ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =        5.0\n",
      "  total_flos               =  1417541GF\n",
      "  train_loss               =     1.6904\n",
      "  train_runtime            = 0:06:10.33\n",
      "  train_samples            =         41\n",
      "  train_samples_per_second =      0.554\n",
      "  train_steps_per_second   =      0.284\n"
     ]
    }
   ],
   "source": [
    "metrics = train_result.metrics\n",
    "\n",
    "\n",
    "metrics[\"train_samples\"] = len(train_dataset)\n",
    "\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7a2107a0-792f-4fd9-bc2f-47f01da0d4e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 219\n",
      "  Batch size = 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='110' max='110' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [110/110 01:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =        5.0\n",
      "  eval_accuracy           =     0.4562\n",
      "  eval_loss               =     3.0792\n",
      "  eval_runtime            = 0:01:08.58\n",
      "  eval_samples            =        219\n",
      "  eval_samples_per_second =      3.193\n",
      "  eval_steps_per_second   =      1.604\n",
      "  perplexity              =    21.7405\n"
     ]
    }
   ],
   "source": [
    "logger.info(\"*** Evaluate ***\")\n",
    "\n",
    "metrics = trainer.evaluate()\n",
    "\n",
    "metrics[\"eval_samples\"] = len(eval_dataset)\n",
    "try:\n",
    "    perplexity = math.exp(metrics[\"eval_loss\"])\n",
    "except OverflowError:\n",
    "    perplexity = float(\"inf\")\n",
    "metrics[\"perplexity\"] = perplexity\n",
    "\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (anaconda3-test)",
   "language": "python",
   "name": "conda-env-anaconda3-test-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
